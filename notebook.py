# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19C1uWGwdO5cSRUHavDSY_cV_fHPxV3h2
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

movies = pd.read_csv('movies.csv')
movies.head()

movie_id = movies['movieId'].tolist()
movie_title = movies['title'].tolist()
movie_genre = movies['genre'].tolist()

print(len(movie_id))
print(len(movie_title))
print(len(movie_genre))

df_movies = pd.DataFrame({
    'id': movie_id,
    'judul': movie_title,
    'genre': movie_genre
})
df_movies

"""### Content Based Filtering (*Count Vectorizer* & *Cosine Similarity*)"""

count_vectorizer = CountVectorizer()
genre_matrix = count_vectorizer.fit_transform(df_movies['genre'])
feature_names = count_vectorizer.get_feature_names_out()

count_matrix_df = pd.DataFrame(
    genre_matrix.toarray(),
    columns=feature_names,
    index=df_movies['judul']
)
count_matrix_df

similarity_matrix = cosine_similarity(genre_matrix, genre_matrix)
similarity_matrix

def recommend_movies(movie_title, similarity_matrix, df_movies):

    movie_index = df_movies[df_movies['judul'] == movie_title].index[0]

    sim_scores = list(enumerate(similarity_matrix[movie_index]))

    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    sim_scores = sim_scores[1:11]

    movie_indices = [i[0] for i in sim_scores]

    recommended_movies = df_movies[['judul', 'genre']].iloc[movie_indices]

    return recommended_movies

movie_title_to_recommend = "Toy Story"
recommended_movies = recommend_movies(movie_title_to_recommend, similarity_matrix, df_movies)
recommended_movies

"""### Collaborative Filtering"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

movies = pd.read_csv('movies.csv')
movies.head()

df = movies[['userId', 'movieId', 'rating']]
df.head()

user_ids = df['userId'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

movie_ids = df['movieId'].unique().tolist()
movie_to_movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded_to_movie = {i: x for i, x in enumerate(movie_ids)}

df_copy = df.copy()
df_copy['user'] = df_copy['userId'].map(user_to_user_encoded)
df_copy['movie'] = df_copy['movieId'].map(movie_to_movie_encoded)

num_users = len(user_to_user_encoded)
print(num_users)

num_movie = len(movie_encoded_to_movie)
print(num_movie)

df_copy['rating'] = df_copy['rating'].values.astype(np.float32)
min_rating = min(df_copy['rating'])
max_rating = max(df_copy['rating'])

print('Number of User: {}, Number of Movie: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_movie, min_rating, max_rating
))

df_copy = df_copy.sample(frac=1, random_state=42)
df_copy

x = df_copy[['user', 'movie']].values

y = df_copy['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * df_copy.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)

print(x, y)

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_movie, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_users = num_users
        self.num_movie = num_movie
        self.embedding_size = embedding_size

        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(num_users, 1)

        self.movie_embedding = layers.Embedding(
            num_movie,
            embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.movie_bias = layers.Embedding(num_movie, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])

        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])

        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)

        x = dot_user_movie + user_bias + movie_bias

        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_movie, embedding_size=50)
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

early_stopping_callback = EarlyStopping(
    patience=5,
    restore_best_weights=True,
    monitor='val_root_mean_squared_error',
    mode='min'
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=100,
    validation_data=(x_val, y_val),
    callbacks=[early_stopping_callback]
)

test_loss, test_rmse = model.evaluate(x, y)
val_loss, val_rmse = model.evaluate(x_val, y_val)

print(f'Test Loss: {test_loss}, Test RMSE: {test_rmse}')
print(f'Validation Loss: {val_loss}, Validation RMSE: {val_rmse}')

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('Model Metrics')
plt.ylabel('RMSE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

model.save('recommendation_model', save_format='tf')

import shutil
from google.colab import files

folder_path = '/content/recommendation_model'
zip_path = '/content/recommendation_model'

shutil.make_archive(zip_path, 'zip', folder_path)

"""### Get Movie Recommendations"""

movies = pd.read_csv('movies.csv')
movies.head()

user_id = movies.userId.sample(1).iloc[0]
movie_visited_by_user = movies[movies.userId == user_id]

movie_not_visited = movies[~movies['movieId'].isin(movie_visited_by_user.movieId.values)]['movieId']
movie_not_visited = list(
    set(movie_not_visited)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_visited = [[movie_to_movie_encoded.get(x)] for x in movie_not_visited]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_visited), movie_not_visited)
)

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_visited[x][0]) for x in top_ratings_indices
]

print('Menampilkan Rekomendasi untuk Pengguna: {}'.format(user_id))
print('===' * 9)
print('Film dengan Rating Tinggi dari Pengguna')
print('----' * 8)

top_movie_user = (
    movie_visited_by_user.sort_values(
        by='rating',
        ascending=False
    )
    .head(5)
    .movieId.values
)

movie_rows = movies[movies['movieId'].isin(top_movie_user)]
for row in movie_rows.itertuples():
    print(row.title, ':', row.genre)

print('----' * 8)
print('10 Rekomendasi Film Teratas')
print('----' * 8)

recommended_movie = movies[movies['movieId'].isin(recommended_movie_ids)]
for row in recommended_movie.itertuples():
    print(row.title, ':', row.genre)